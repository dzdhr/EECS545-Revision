{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EECS 545 - Machine Learning\n",
    "## Lecture 3: Convex Optimization + Probability/Stat Overview\n",
    "### Date: January 13, 2016\n",
    "### Instructor: Jacob Abernethy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML, Image\n",
    "from IPython.display import YouTubeVideo\n",
    "from sympy import init_printing, Matrix, symbols, Rational\n",
    "import sympy as sym\n",
    "from warnings import filterwarnings\n",
    "init_printing(use_latex = 'mathjax')\n",
    "filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some important notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* HW1 is out! Due January 25th at 11pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Homework will be submitted via *Gradescope*. Please see Piazza for precise instructions. Do it soon, not at the last minute!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There is an *optional* tutorial **this evening**, 5pm, in Dow 1013. Come see Daniel go over some tough problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* No class on Monday January 18, MLK day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python: We recommend Anacdona\n",
    "<img src=\"https://www.continuum.io/sites/all/themes/continuum_foundation/images/logos/logo-anaconda.svg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "* Anaconda is standalone Python distribution that includes all the most important scientific packages: *numpy, scipy, matplotlib, sympy, sklearn, etc.*\n",
    "* Easy to install, available for OS X, Windows, Linux.\n",
    "* Small warning: it's kind of large (250MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some notes on using Python\n",
    "* HW1 has only a very simple programming exercise, just as a warmup. We don't expect you to submit code this time\n",
    "* This is a good time to start learning Python basics\n",
    "* There are **a ton** of good places on the web to learn python, we'll post some\n",
    "* Highly recommended: **ipython**; it's a much more user friendly terminal interface to python\n",
    "* Even better: **jupyter notebook**, a web based interface. This is how I'm making these slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking if all is installed, and HelloWorld\n",
    "If you got everything installed, this should run:\n",
    "```python\n",
    "\n",
    "# numpy is crucial for vectors, matrices, etc.\n",
    "import numpy as np              \n",
    "# Lots of cool plotting tools with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# For later: scipy has a ton of stats tools\n",
    "import scipy as sp\n",
    "# For later: sklearn has many standard ML algs\n",
    "import sklearn\n",
    "# Here we go!\n",
    "print(\"Hello World!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More on learning python\n",
    "* We will have one tutorial devoted to this\n",
    "* If you're new to Python, go slow!\n",
    "    * First learn the basics (lists, dicts, for loops, etc.)\n",
    "    * Then spend a couple days playing with numpy\n",
    "    * Then explore matplotlib\n",
    "    * etc.\n",
    "* Piazza = your friend. We have a designated python instructor (IA Ben Bray) who has lots of answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture Cat #2\n",
    "![](http://i.giphy.com/12bfinkUSz2jGE.gif)\n",
    "(credit to Johann for the suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functions and Convexity\n",
    "* Let $f$ be a function mapping $\\mathbb{R}^{n} \\to \\mathbb{R}$, and assume $f$ is twice differentiable.\n",
    "* The *gradient* and *hessian* of $f$, denoted $\\nabla f(x)$ and $\\nabla^2 f(x)$, are the vector an matrix functions:\n",
    "$$\\nabla f(x) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_1}\\end{bmatrix} \\quad \\quad \\quad \\nabla^2 f(x) = \\begin{bmatrix}\\frac{\\partial^2 f}{\\partial x_1^2} & \\ldots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\  \\vdots & & \\vdots \\\\\\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} & \\ldots & \\frac{\\partial^2 f}{\\partial x_n^2}\\end{bmatrix}$$\n",
    "* Note: the hessian is always symmetric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex functions\n",
    "* We say that a function $f$ is *convex* if, for any distinct pair of points $x,y$ we have\n",
    "$$f\\left(\\frac{x + y}{2}\\right) \\leq \\frac{f(x)}{2} + \\frac{f(y)}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "<p align=\"center\"><img src='http://www.probabilitycourse.com/images/chapter6/Convex_b.png' width=\"400px\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fun facts about convex functions\n",
    "* If $f$ is differentiable, then $f$ is convex iff $f$ \"lies above its linear approximation\", i.e.:\n",
    "$$ f(x + y) \\geq f(x) + \\nabla_x f(x) \\cdot y \\quad \\text{ for every } x,y$$\n",
    "* If $f$ is twice-differentiable, then the hessian is always positive semi-definite!\n",
    "* This last one you will show on your homeowork :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Sets\n",
    "- $C \\subseteq \\mathbb{R}^n$ is **convex** if\n",
    "$$t x + (1-t)y \\in C$$\n",
    "for any $x, y \\in C$ and $0 \\leq t \\leq 1$\n",
    "- that is, a set is convex if the line connecting **any** two points in the set is entirely inside the set\n",
    "\n",
    "![](http://chaojiang.me/wp-content/uploads/2015/01/Screenshot-from-2015-01-29-154831.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not all sets are convex\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/1f/Non_Convex_set.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Most General Optimization Problem\n",
    "Assume $f$ is some function, and $C \\subset \\mathbb{R}^n$ is some set. The following is an *optimization problem*:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{minimize} & f(x) \\\\\n",
    "\\mbox{subject to} & x \\in C\n",
    "\\end{array}\n",
    "$$\n",
    "* How hard is it to find a solution that is (near-) optimal? This is one of the fundamental problems in Computer Science and Operations Research.\n",
    "* A huge portion of ML relies on this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A rough optimization hierarchy\n",
    "$$\n",
    "\\mbox{minimize } f(x) \\quad \\mbox{subject to } x \\in C\n",
    "$$\n",
    "* **[Really Easy]** $C = \\mathbb{R}^n$ (i.e. problem is *unconstrained*), $f$ is convex, $f$ is differentiable, strictly convex, and \"slowly-changing\" gradients\n",
    "* **[Easyish]** $C = \\mathbb{R}^n$, $f$ is convex\n",
    "* **[Medium]** $C$ is a convex set, $f$ is convex\n",
    "* **[Hard]** $C$ is a convex set, $f$ is non-convex\n",
    "* **[REALLY Hard]** $C$ is an arbitrary set, $f$ is non-convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization without constraints\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{minimize} & f(x) \\\\\n",
    "\\mbox{subject to} & x \\in \\mathbb{R}^n\n",
    "\\end{array}\n",
    "$$\n",
    "* This problem tends to be easier than constrained optimization\n",
    "* We just need to find an $x$ such that $\\nabla f(x) = \\vec{0}$\n",
    "* Techniques like *gradient descent* or *Newton's method* work in this setting. (More on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization with constraints\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{minimize} & f(x) \\\\\n",
    "\\mbox{subject to} & g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{array}\n",
    "$$\n",
    "* Here the set $C := \\{ x : g_i(x) \\leq 0, i=1, \\ldots, m \\}$\n",
    "* $C$ is convex as long as all $g_i(x)$ convex\n",
    "* The solution of this optimization may occur in the *interior* of $C$, in which case the optimal $x$ will have $\\nabla f(x) = 0$\n",
    "* But what if the solution occurs on the *boundary* of $C$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://staff.www.ltu.se/~larserik/applmath/chap7en/lagrange.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Quick Overview of Lagrange Duality\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{minimize} & f(x) \\\\\n",
    "\\mbox{subject to} & g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{array}\n",
    "$$\n",
    "* Here we need to work with the *Lagrangian*:\n",
    "$$\n",
    "L(x,\\boldsymbol{\\lambda}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x)\n",
    "$$\n",
    "* The vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^m$ are *dual variables*\n",
    "* For fixed $\\boldsymbol{\\lambda}$, we now solve $\\nabla_x L(x,\\boldsymbol{\\lambda}) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Quick Overview of Lagrange Duality\n",
    "$$\n",
    "\\text{Lagrangian: } \\quad L(x,\\boldsymbol{\\lambda}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x)\n",
    "$$\n",
    "* Assume, for every fixed $\\lambda$, we found $x_\\lambda$ such that\n",
    "$$ \\nabla_x L(x_\\lambda,\\boldsymbol{\\lambda}) =  \\nabla f(x_\\lambda) + \\sum_{i=1}^m \\lambda_i \\nabla g_i(x_\\lambda) = \\mathbf{0}$$\n",
    "* Now we have what is called the *dual function*, $$h(\\boldsymbol{\\lambda}) := \\inf_{x \\in \\mathbb{R}^n} L(x,\\boldsymbol{\\lambda}) =  L(x_\\lambda,\\boldsymbol{\\lambda})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Lagrange Dual Problem\n",
    "* What did we do here? We took one optimization problem:\n",
    "$$\n",
    "p^* := \\begin{array}{ll}\n",
    "\\mbox{minimize} & f(x) \\\\\n",
    "\\mbox{subject to} & g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* And then we got another optimization problem:\n",
    "$$\n",
    "d^* := \\begin{array}{ll}\n",
    "\\mbox{maximize} & h(\\boldsymbol \\lambda) \\\\\n",
    "\\mbox{subject to} & \\lambda_i \\geq 0, \\quad i = 1, \\ldots, m\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Sometimes this *dual problem* is easier to solve\n",
    "* We always have *weak duality*: $p^* \\geq d^*$\n",
    "* Under nice conditions, we get *strong duality*: $p^* = d^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended reading:\n",
    "* Free online!\n",
    "* Chapter 5 covers duality\n",
    "<img src=\"http://stanford.edu/~boyd/cvxbook/bv_cvxbook_cover.jpg\" width=\"300px\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
